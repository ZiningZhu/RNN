{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vocab2vec(vocab_size, vocab_length=10**7):\n",
    "    f = open(\"Combined_String.txt\", \"r\")\n",
    "    s = f.read()\n",
    "    f.close()\n",
    "    D = 'abcdefghijklmnopqrstuvwxyz .,\\'1234567890\";'\n",
    "    res = []\n",
    "    for i in range(vocab_length):\n",
    "        c = s[i].lower()\n",
    "        v = np.zeros((vocab_size))\n",
    "        try:\n",
    "            idx = D.index(c)\n",
    "            v[idx] = 1\n",
    "            res.append(v)\n",
    "        except (ValueError, IndexError) as e:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "    ret = np.array(res) # A list of shape (vocab_length,) one-hot encoded characters\n",
    "    print (\"shape is: {}\".format(ret.shape))\n",
    "    return ret\n",
    "\n",
    "#vocab2vec(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# What is LSTM? #  \n",
    "- [Nico's blog on LSTM](http://nicodjimenez.github.io/2014/08/08/lstm.html)  \n",
    "- [Colah's blog on LSTM](http://colah.github.io/posts/2015-08-Understanding-LSTMs)  \n",
    "- Written in equation:\n",
    "    Each LSTM cell has three inputs (character $x_t$, cell prediction $h_{t-1}$, and hidden state $C_{t-1}$) and two outputs (hidden state $C_t$ and cell prediction $h_t$).  \n",
    "    Forget gate: \n",
    "    $$f_t = \\sigma (W_f [h_{t-1}, x_t] + b_f)$$   \n",
    "    Information gate: \n",
    "    $$i_t = \\sigma (W_i [h_{t-1}, x_t] + b_i)$$  \n",
    "    Updates for cell state:\n",
    "    $$D_t = tanh (W_D [h_{t-1}, x_t] + b_D)$$  \n",
    "    $$C_t = f_t * C_{t-1} + i_t * D_t$$\n",
    "    Output layers:  \n",
    "    $$o_t = \\sigma (W_o [h_{t-1}, x_t] + b_o)$$\n",
    "    $$h_t = o_t * tanh(C_t)$$\n",
    "    \n",
    "- Training goal:  \n",
    "    $argmin_W J$, where\n",
    "    $$J = \\sum_t (y_t log h_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LSTM used for reading paragraphs character by character**\n",
    "- Cell prediction is the next (batch of) character, given the input and previous states.\n",
    "- The first character in prediction sequence is used to calculate the cross entropy.\n",
    "\n",
    "\n",
    "**Existing code examples:**  \n",
    "- [Aymeric Damien's TensorFlow-Examples](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/recurrent_network.py)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# My implementation of LSTM on character reading - based on the equation above\n",
    "\n",
    "class BasicLSTM:\n",
    "    def __init__(self, vocab_size, cell_size, batch_size, continue_training = False, global_step = -1):\n",
    "        self.batch_size = batch_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.cell_size = cell_size\n",
    "        self.global_step = global_step\n",
    "        \n",
    "        self.global_step = global_step\n",
    "        self.MODEL_NAME = \"./model/LSTM\"\n",
    "        self.TEST_SAMPLE_SEQ_LENGTH = 100\n",
    "        self._construct_networks(vocab_size, cell_size, batch_size, continue_training, global_step)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def _weight(self, shape, dtype=tf.float32, name=None):\n",
    "        m = 0\n",
    "        s = 0.01\n",
    "        return tf.Variable(tf.random_normal(shape=shape, mean=m, stddev=s, dtype=dtype), dtype, name=name)\n",
    "    \n",
    "    def _const(self, shape, name, dtype=tf.float32):\n",
    "        d0 = shape[0]\n",
    "        d1 = shape[1]\n",
    "        tmp = np.zeros(shape=shape)\n",
    "        tmp[:, 0] = np.ones(shape=[d0, 1])\n",
    "        return tf.constant(tmp, dtype=dtype, name=name)\n",
    "    \n",
    "    def _ohe2char(self, ohe_vec): # takes only the first row in ohe_vec\n",
    "        assert ohe_vec.shape[1] == self.vocab_size\n",
    "        chars = 'abcdefghijklmnopqrstuvwxyz .,\\'1234567890\";'\n",
    "        choice_id = np.random.choice(self.vocab_size, p=ohe_vec[0,:].ravel())\n",
    "        return chars[choice_id]\n",
    "            \n",
    "\n",
    "    def _construct_networks(self, vocab_size, cell_size, batch_size, continue_training, global_step):\n",
    "        graph = tf.Graph()\n",
    "        with graph.as_default():\n",
    "            x = tf.placeholder(tf.float32, [batch_size, vocab_size], name=\"x\")\n",
    "            y = tf.placeholder(tf.float32, [batch_size, vocab_size], name=\"y\")\n",
    "            init_C = tf.placeholder(tf.float32, [batch_size, cell_size], name=\"init_C\")\n",
    "            init_h = tf.placeholder(tf.float32, [batch_size, vocab_size], name=\"init_h\")\n",
    "        \n",
    "            if not continue_training:\n",
    "                # Fotget gate\n",
    "                Wf = self._weight([2 * vocab_size, cell_size], name=\"Wf\")\n",
    "                bf = self._const([1, cell_size], name=\"bf\")\n",
    "                f = tf.nn.softmax(tf.matmul(tf.concat([init_h, x], axis=1), Wf) + bf, dim=1)\n",
    "\n",
    "                # Info gate\n",
    "                Wi = self._weight([2 * vocab_size, cell_size], name=\"Wi\")\n",
    "                bi = self._const([1, cell_size], name=\"bi\")\n",
    "                i = tf.nn.softmax(tf.matmul(tf.concat([init_h, x], axis=1), Wi) + bi, dim=1)\n",
    "\n",
    "                # Next cell state\n",
    "                Wd = self._weight([2 * vocab_size, cell_size], name=\"Wd\")\n",
    "                bd = self._const([1, cell_size], name=\"bd\")\n",
    "                D = tf.tanh(tf.matmul(tf.concat([init_h, x], axis=1), Wd) + bd)\n",
    "\n",
    "                # Update cell state\n",
    "                C = tf.add(f * init_C, i * D, name=\"C\")\n",
    "\n",
    "                # Output layers\n",
    "                Wo = self._weight([2 * vocab_size, vocab_size], name=\"Wo\")\n",
    "                bo = self._const([1, vocab_size], name=\"bo\")\n",
    "                o = tf.nn.softmax(tf.matmul(tf.concat([init_h, x], axis=1), Wo) + bo, dim=1)\n",
    "                h = tf.multiply(o, tf.tanh(C), name=\"h\")\n",
    "                hs = tf.nn.softmax(h)\n",
    "\n",
    "                # Loss function, etc.\n",
    "                #hs = tf.nn.softmax(h, dim=1) # Convert h into softmax form\n",
    "                loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=h, name=\"loss\"))\n",
    "                optimizer = tf.train.AdamOptimizer()\n",
    "                grad_limit = tf.constant(5.0, dtype=tf.float32, name=\"grad_limit\")\n",
    "                grads_and_vars = optimizer.compute_gradients(loss)\n",
    "                clipped_grads_and_vars = []\n",
    "                for grad, var in grads_and_vars:\n",
    "\n",
    "                    clipped_grad = tf.clip_by_value(grad, -grad_limit, grad_limit)\n",
    "                    clipped_grads_and_vars.append((clipped_grad, var))\n",
    "                train_step = optimizer.apply_gradients(clipped_grads_and_vars, name=\"train\")\n",
    "\n",
    "\n",
    "                # Session, Saver, etc.\n",
    "                saver = tf.train.Saver()\n",
    "                sess = tf.Session()\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "                saver.save(sess, self.MODEL_NAME,global_step=0)\n",
    "\n",
    "            else:\n",
    "                sess = tf.Session()\n",
    "\n",
    "                saver = tf.train.import_meta_graph(self.MODEL_NAME + \"-{}.meta\".format(global_step))\n",
    "                saver.restore(sess,tf.train.latest_checkpoint('./'))\n",
    "\n",
    "                graph = tf.get_default_graph()\n",
    "\n",
    "                x = graph.get_tensor_by_name(\"x:0\")\n",
    "                y = graph.get_tensor_by_name(\"y:0\")\n",
    "                init_C = graph.get_tensor_by_name(\"init_C:0\")\n",
    "                init_h = graph.get_tensor_by_name(\"init_h:0\")\n",
    "                C = graph.get_tensor_by_name(\"C:0\")\n",
    "                h = graph.get_tensor_by_name(\"h:0\")\n",
    "                Wf = graph.get_tensor_by_name(\"Wf:0\")\n",
    "                Wi = graph.get_tensor_by_name(\"Wi:0\")\n",
    "                Wd = graph.get_tensor_by_name(\"Wd:0\")\n",
    "                Wo = graph.get_tensor_by_name(\"Wo:0\")\n",
    "                J = graph.get_tensor_by_name(\"J:0\")\n",
    "\n",
    "                train_step = graph.get_tensor_by_name(\"train:0\")\n",
    "            \n",
    "        \n",
    "        # After creation, save to class variables\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.init_C = init_C\n",
    "        self.init_h = init_h\n",
    "        self.C = C\n",
    "        self.h = h\n",
    "        self.hs = hs\n",
    "        self.loss = loss\n",
    "        self.train_step = train_step\n",
    "        self.saver = saver\n",
    "        self.sess = sess\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "    def train(self, steps, training_data, sample = True, sample_every = 200000):\n",
    "        save_per_steps = 10\n",
    "        batch_size = self.batch_size\n",
    "        vocab_size = self.vocab_size\n",
    "        cell_size = self.cell_size\n",
    "        \n",
    "        for stp in range(steps):\n",
    "            \n",
    "            prev_C = np.random.rand(batch_size, cell_size)\n",
    "            prev_h = np.random.rand(batch_size, vocab_size)\n",
    "            p = 0\n",
    "            while p < (len(training_data) - batch_size - 1):\n",
    "                self.global_step\n",
    "                fdata = {self.init_C: prev_C, \n",
    "                         self.init_h: prev_h,\n",
    "                         self.x: training_data[p : p + batch_size], \n",
    "                         self.y: training_data[p+1 : p+1+batch_size]\n",
    "                         }\n",
    "                _, next_C, next_h, loss = self.sess.run([self.train_step, self.C, self.h, self.loss], feed_dict = fdata)\n",
    "                \n",
    "                p += batch_size\n",
    "                \n",
    "            \n",
    "                if sample and p % sample_every == 0:\n",
    "                    # Perform a trial of sample run \n",
    "                    words_outputs = \"\"\n",
    "                    for i in range(self.TEST_SAMPLE_SEQ_LENGTH):\n",
    "                        fdata = {self.init_C: prev_C,\n",
    "                                self.init_h: prev_h,\n",
    "                                self.x: training_data[p : p + batch_size],\n",
    "                                 self.y: training_data[p+1 : p+1+batch_size]\n",
    "                                }\n",
    "                        _, vec_ohe, loss = self.sess.run([self.C, self.hs, self.loss], feed_dict = fdata)\n",
    "\n",
    "                        words_outputs += self._ohe2char(vec_ohe)\n",
    "                    \n",
    "                    print (\"--- n = {}, p = {}, loss = {} ---\".format(self.global_step, p, loss))\n",
    "                    print (\"{}\\n\".format(words_outputs))\n",
    "                    \n",
    "                prev_C = next_C\n",
    "                prev_h = next_h\n",
    "                    \n",
    "            if self.global_step % save_per_steps == 0:\n",
    "                self.saver.save(self.sess, self.MODEL_NAME, global_step = self.global_step)\n",
    "        \n",
    "            self.global_step += 1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started!\n",
      "shape is: (992170, 40)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print (\"Started!\")\n",
    "    lstm = BasicLSTM(vocab_size = 40,\n",
    "                    cell_size = 40, # They have to be equal. GGWP\n",
    "                    batch_size = 10,\n",
    "                    continue_training = False,\n",
    "                    global_step = -1)\n",
    "    training_words = vocab2vec(40, 10 ** 6)\n",
    "    lstm.train(steps = 10**5, training_data = training_words, sample = True)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started!\n",
      "shape is: (9919422, 40)\n",
      "--- n = -1, p = 200000, loss = 2.95654296875 ---\n",
      "g6cj 7bjyxwcgv8'skgr1h3l 6 29x7c2okfq'.ougq911qe2ank5tl'nrivbx5h1p5s5yyt bb2v'8cl,jewnwsadh9  r2exbx\n",
      "\n",
      "--- n = -1, p = 400000, loss = 3.7170217037200928 ---\n",
      "dcldp za7pwjm2'5gpd8ezej1ai'lohyl3x65pema7wk,.rk.5jstjumcs9p60qcgk3e,q, t5agg.i'wvo9ksb2 u.ajkhar'03\n",
      "\n",
      "--- n = -1, p = 600000, loss = 2.955446720123291 ---\n",
      "n1sex xdbq1't4w2yn85e,3l9a88pfkt20zfkdvf.pm1qtyu4sr2wvh69bxokibz.5dwjnlf,3yfbbi5pbx168l34k2xi5dqpc ,\n",
      "\n",
      "--- n = -1, p = 800000, loss = 3.6754627227783203 ---\n",
      "f7jxxixyyyvjtb5quwbmh2.g9widj3uublx0dm11aq'00oztb9jccd wum0 o p9a1y9jfosdg4au5z3whja,vq5o 7o5tc6b1jm\n",
      "\n",
      "--- n = -1, p = 1000000, loss = 3.672924041748047 ---\n",
      "mus k.86yj21hv0 vy5iv97dsv  27chhs wfcivme5f08j9k2z4tl4yxgmbtd4po5ta95vst,en82av3v785j8qbttx8x7bscsn\n",
      "\n",
      "--- n = -1, p = 1200000, loss = 3.7308573722839355 ---\n",
      "tba,a1ha93e35o'i.bnpfztelsle6a4,sp323swh jw2dtfysq wtlr6b3yxxseuq5psefyj960'5zqzwzqgx y0q8'l1g7fbxg.\n",
      "\n",
      "--- n = -1, p = 1400000, loss = 3.6754627227783203 ---\n",
      "v6tw l.b0'fw94ajn0q7xc.pu48ew,axunp9kj97nqxt1sg217r0i14q7o5b,akfruqbri,pnsit3zbyl2cs6spf6so6'p9gxnq1\n",
      "\n",
      "--- n = -1, p = 1600000, loss = 3.7286341190338135 ---\n",
      "e41rz. aq2ntk03,q9mk0g4croujn7wns84l9'ysyui0htx2wyg2 nlxlu8jh8pk97wutd3z bb87 'ye'dg'.8quhvrk1oh1wf8\n",
      "\n",
      "--- n = -1, p = 1800000, loss = 3.7286343574523926 ---\n",
      "h3mhjqyj1aq3wpm  xmrbsljly.lx72u5l5tbn60jlaqu3.fo2m 4445k7bu5yqbgp70'ya3d,yn gggta1bjdsh7z'ywivz7jz5\n",
      "\n",
      "--- n = -1, p = 2000000, loss = 2.735563039779663 ---\n",
      "7798ddsu 8 y9ris'ww94p'mew 5k3du6w9  sjsvo9pnnn46z3i1d7dq y0is3c   wzcgohr3aprp8dl nfb61w.dsy9w'h053\n",
      "\n",
      "--- n = -1, p = 2200000, loss = 3.7286345958709717 ---\n",
      "mn mpymvl09xjzp' k m2'7q8yu.pj604doj9imy1i,55x2aohm,.54ughz,ija 'c jfo329e15f1 jlt,3's1v4evazy .poaa\n",
      "\n",
      "--- n = -1, p = 2400000, loss = 3.7170169353485107 ---\n",
      "6839bar0wggbe2po,rcc3b4 babokj763biv.637n73zz6c8w4mi.as2ygnk.4c3hiad'e3ylid 605m44a'65csq6d7p72ayy's\n",
      "\n",
      "--- n = -1, p = 2600000, loss = 3.675487756729126 ---\n",
      "c4vhjksddg7vzoyz'lkuq9xd6dwjvd's,5f.9'8.gbtlc,otyvsjkq2icl9.2e97joztm74j3g.wspibi 5lc1edmttw'x74wjjq\n",
      "\n",
      "--- n = -1, p = 2800000, loss = 3.6754627227783203 ---\n",
      "711amgh5lm57,3k''w1tyvcui 1tb684q05utcu1si,ca,fkstgcfe1e9owy3 .8wnkj8scc9aev4ao1nvhp2m2z 6iwg,hvsnk1\n",
      "\n",
      "--- n = -1, p = 3000000, loss = 3.7170002460479736 ---\n",
      "lml,4um,0qhbo7temjkab42f'5.pxozjzsiuggvnx4rfi6 ,iofo4vfqbakssp''6s8drxuke6ksinh5l70gq8wwt29ab.eb5n7d\n",
      "\n",
      "--- n = -1, p = 3200000, loss = 3.717021942138672 ---\n",
      "879'n bgvq82ff93k70xq797txwc9r8ilbry, i2o'e4,k83otekdj i,44'yx'8 qbzfzfcxq5q0ha'gu5 p,g,zjw9lr 6 s8u\n",
      "\n",
      "--- n = -1, p = 3400000, loss = 3.673292398452759 ---\n",
      ".dqy uig,,nvi.fq1hn8z6.ipnkoqj'qcecns0d,v6feh,f5p8mi.sqm6mg.f2zk.6un2h6ivq2kws.aqcy,7'd2w55ii2i26kq8\n",
      "\n",
      "--- n = -1, p = 3600000, loss = 3.6754627227783203 ---\n",
      "vltn7wp0l90p'0ya1b768hx7y.q1.'jsk qxqtwpboh9ktj.jjw4b7'5556r.nay7ysphnba63zdhz39gzo4mrzv  ya6n4'jsus\n",
      "\n",
      "--- n = -1, p = 3800000, loss = 2.838205575942993 ---\n",
      " vqbj3.2, 1fupceqjnlz9mjg2tvjac49epmo bi,h.5 hoaan5xzsu8yroxht5u8doytzpt.to3x6bbhx'm4tx5t'plps7bxmi0\n",
      "\n",
      "--- n = -1, p = 4000000, loss = 3.717021942138672 ---\n",
      "viuky34zd2b 8 7vppk07q,'ie4sci pk.2mo a '8gfa31o808ensvphbz.fako ,zcbjfuw,yl1h4dcssqut0cfmt,2sb63kv3\n",
      "\n",
      "--- n = -1, p = 4200000, loss = 3.6754863262176514 ---\n",
      "1a.d8no50 z kcj0tgdf4pvcrcd 9.ff4bku7wqszp kpturasn4kiw,1s3 znnb,3'r3rj8g85hv95cmh0g4729ju1cc4q.f32q\n",
      "\n",
      "--- n = -1, p = 4400000, loss = 3.717021942138672 ---\n",
      "ggo5m9'x0  a.xzcn0wz7qbc.8z5cn1z2hffh,'vz cv.os.1 ih kszdz7b0c3lv8bvfcg6.ngb.fz6hv5ziyao2g   yz npm'\n",
      "\n",
      "--- n = -1, p = 4600000, loss = 2.764331102371216 ---\n",
      "qy9cpkpkj75 703 a20'9dt5sca52yid cmgs2x 1ox0.xd9.s4fpa0lw,tdr0xxjd '4mq67x'z yhjtm.d59oyoxn's3ez44 8\n",
      "\n",
      "--- n = -1, p = 4800000, loss = 2.955427646636963 ---\n",
      ".cgbl0vlf.ttjo3fdnot06schywapf5077fuopecaebwnih8z jir9b6c0'.056a weyxxy.li9wnynurrye1ecit6403sd499y7\n",
      "\n",
      "--- n = -1, p = 5000000, loss = 3.7146689891815186 ---\n",
      "r ht y.eb0r5qq c 4yv.ugwk,h4lxoxgxi4os,fw3ml0q94i117mhm7m32on.mo0z7f1rqtb k0csk,'8m,r pv3ev4fhg9zxw8\n",
      "\n",
      "--- n = -1, p = 5200000, loss = 3.7170214653015137 ---\n",
      "g7gqm.jkfj2,962'0'lmkt6y3zikcf5p95q1eh6sxn55xvwe9u7qewxnude''16jhrv7,9glpv 26lyueg2,3kp10o5w2nie.d1p\n",
      "\n",
      "--- n = -1, p = 5400000, loss = 3.6754627227783203 ---\n",
      "hqy'1nm9bqnljcopwg2xns40es1kipefxmr0f1s d7cbmhzkfcr'p7.z48lqnm2wud4bml1c19iu9tafb4'4tnvtm4dvk3 864,,\n",
      "\n",
      "--- n = -1, p = 5600000, loss = 2.955427646636963 ---\n",
      "gei.uaeu0020,sxmz,d64v2 vkp'0lznjv1hq,ssysu,e1oe73t6r.v40pdd''ocgd6x26x05tkij931fcws811907gj0zaorkd,\n",
      "\n",
      "--- n = -1, p = 5800000, loss = 3.7306180000305176 ---\n",
      "0p0 qzuxo g6 ragf0r9c2zyc c0c5c.nv tgl9ql'mksl93zv.38tauy.tf9hhs j'g2so3r ywrh1ka 4gpupqexj2thflif s\n",
      "\n",
      "--- n = -1, p = 6000000, loss = 2.955432176589966 ---\n",
      "702515ibw.o 743z2qu2oe4'6jaxc9igpsi74j0vebbnj ky 9 webrzqybf xd27l'7trv7de478ehcsc 0f p6em0qzd4l2u  \n",
      "\n",
      "--- n = -1, p = 6200000, loss = 3.6754648685455322 ---\n",
      "nvnjtj0mrdrp0av1u11c9qxuk5u687oc5ij9srov8lkfwdux0pkwpfupk'349x'330c,aa4ej,zx7ulv37h7'wb,11,i8kotk'rj\n",
      "\n",
      "--- n = -1, p = 6400000, loss = 3.7286558151245117 ---\n",
      ",9cke57ylo bjdjkgyz9pci9vc2', rtou8mxceoy.4auvin5'iogk5nr'goj8kk5aiw,t.3r,6avw2x.qa8erxwzqr.m7v,zqev\n",
      "\n",
      "--- n = -1, p = 6600000, loss = 3.6754631996154785 ---\n",
      "9h0ayzq5e1p07x24whttl3,3mua21x5aukijpl7p3002493lxwvbxnz378pq1122h9hpsvctw9 mk36w2dvehs4urri8hz5n5zw8\n",
      "\n",
      "--- n = -1, p = 6800000, loss = 3.7286345958709717 ---\n",
      "1ke',96k,mrq3atpjgybgakxdajbv3y2783ewqxgllicbt5puxnpbqa'j21e9pz2n3c9h6w0qdj9'v,wp5a3 ayca3mj '1g4a3y\n",
      "\n",
      "--- n = -1, p = 7000000, loss = 3.7170181274414062 ---\n",
      "dklu5.9he 7am47iydqag.03c1o.5ig ot1 nii3bd00i7gx8nrmc9pda4.wb6u,2zufgbex1oq3oe176q9rcav2obizyws8.qm \n",
      "\n",
      "--- n = -1, p = 7200000, loss = 3.7170166969299316 ---\n",
      "mhef13lcyn ,9ighayw0n6,.x679 tzkv1.4kjnx'ss7u52ddbvo3vetz1k0ux5zy.sekk'dvp36klhdip881tkesl1l4cpep3dr\n",
      "\n",
      "--- n = -1, p = 7400000, loss = 3.717021942138672 ---\n",
      "tsiwz2 4m5zogp',0 l vli'fzd t6ig j 8w9nt1974o1h2zylumw2ywnsihtg87zqp vouh4izj7m'brkzry m2q352ucvuvrg\n",
      "\n",
      "--- n = -1, p = 7600000, loss = 2.7646069526672363 ---\n",
      "paozn.iuqvjfiq.yw5lpt9xpfa6 tftajbig6jfkplw 8etur wpzyinx1ym87 81klbxaou60bhbkl7bwjezf7319.k bp'cncm\n",
      "\n",
      "--- n = -1, p = 7800000, loss = 3.7286345958709717 ---\n",
      "br3sp j53dhqt.8hb5pwc2bq7l e9to'o32tu s x32ojbi.rmwggo ..,i y 2f,.w3n.jhq60 4e.3pov0bd7jp0.'hsaha30a\n",
      "\n",
      "--- n = -1, p = 8000000, loss = 2.955427646636963 ---\n",
      "5qiscmhn63nh0od j2 5op  m5ov9gb8xb,7d2f mu7nw7wq,qrm5t3 ci4fun0 tn71sy3fe n9eqaszz1y1y0ru12mbp,8lpol\n",
      "\n",
      "--- n = -1, p = 8200000, loss = 3.7170193195343018 ---\n",
      "vvnn.pgt9lbh1xsi'p6t1nuc.1e5mnrchns1nfg4af2zobm73uqm''saj'w5vntyctsq4erinr a0snfgkesf3roz9.ytq79w9o,\n",
      "\n",
      "--- n = -1, p = 8400000, loss = 3.6754627227783203 ---\n",
      "c 2r dm kjtuxd62dl7hpwfi6,87k4z6,dt'pbx7bo4k3xqv0d7suiuo8yagc4p2 v382krm'grd5obyalppy.oztaeqk wbvk9o\n",
      "\n",
      "--- n = -1, p = 8600000, loss = 3.7170217037200928 ---\n",
      "8junrkapbewim pe4r1dugqjx5xvwzuoxyeslggt53rlnu4aa5'jp3t31ex56ira xce dusxunhna4391gvaq4l6j901.k42y7u\n",
      "\n",
      "--- n = -1, p = 8800000, loss = 3.717021942138672 ---\n",
      "x'4t,f9f,rqh5yzdpeushc7uzprvjvudfs4tjjyju,9k135x 8jqj,w. 7vpxfdueaemnvr4n7b2xgea0c9,,5wtgogfvz,ihc 8\n",
      "\n",
      "--- n = -1, p = 9000000, loss = 3.730933427810669 ---\n",
      "dmy 676i29g2jcgqji80e.1dsf769a6 o9r,7nw j',rn6n0zj80 cjo7im5rijxchl6sf5aa, o6rw9z6k8 hq1cwnk0sjeiqsw\n",
      "\n",
      "--- n = -1, p = 9200000, loss = 3.7170217037200928 ---\n",
      "un21zofn32b.ypqx.18jn9vk0z'.t pl6lt.u7fxqrycc7d'4so'rl3re6z,qz3b9hsh7g3'54tvx15d,ycqm69v 0 h64vhbn09\n",
      "\n",
      "--- n = -1, p = 9400000, loss = 3.7286343574523926 ---\n",
      "wunaoogwghvc tov5drfnkxxi,hfl7.z wkakvfb03aeqszdapxq'v,dqxu,tpv8a.rprf7 noykla0u7wi e 4abz,69pk37wwa\n",
      "\n",
      "--- n = -1, p = 9600000, loss = 3.717021942138672 ---\n",
      "pk.gf.on'knd.,mwlmdily,9ijcvz2q6dlfljcs,lqo,9z.xe1 z74ichw7cd6vjt98eb8xnl1ff,j96.wz.m cnwrcvm,pu8l.n\n",
      "\n",
      "--- n = -1, p = 9800000, loss = 3.6754631996154785 ---\n",
      "l',rz667ni 7d0q8'.jz.ok'gfuw8b.2dycybf8tkq1eqib ll4wzsw04n3hu49f08ngg,p1jy3q3n'46ug9v.ya,k89n18vkzp \n",
      "\n",
      "--- n = 0, p = 200000, loss = 2.955427646636963 ---\n",
      "d'4rz7xza26z97lwe jk5pu5th86 wlnb4',lzcojix4,.2nsphv7d21qnxfsv3p'2,,2i'xujqr l5oy6h3i 'dws9r,7oihz e\n",
      "\n",
      "--- n = 0, p = 400000, loss = 3.717021942138672 ---\n",
      ",6cpq8z7 p2vlcuwt8eh1dc1ifk'0gpqvairso'x6paa4ha,eyo,t6rma92b.e,w6c5'64yuuagaj70qmub,1uq7i0z4fkva5q7r\n",
      "\n",
      "--- n = 0, p = 600000, loss = 2.955427646636963 ---\n",
      "74srji6yto9,sf''ha6j4qfqstjls9q820yfqeujdia5k'd..b 58 ,v1e 4,6  o3u4bz,hf4o lxo.jkf1a8j3hlcgxc9pd  n\n",
      "\n",
      "--- n = 0, p = 800000, loss = 3.6754629611968994 ---\n",
      "r6,'6lfog55utb76vpvsx8 .fgoo2siydty7 ,qe1 9517kc 19''g24g'9t7t2uh''l0u ,v3lmjgudrr'6vk1z8xby8932mo59\n",
      "\n",
      "--- n = 0, p = 1000000, loss = 3.672919273376465 ---\n",
      " , y9w2i0r'm6d0,s'tplxpzy73ub2y6i5,,5srx,hbnpzft9x.i2o86n5a9aotgtqannzp6l,864fhbl5z76ob,05zqhqk6yy1w\n",
      "\n",
      "--- n = 0, p = 1200000, loss = 3.7309322357177734 ---\n",
      " 7a4m01st3o8rhx ufbv7zy'bstcoulggq60otsmk.s4. w4boefh ewps8gs.6qo'ozkd i7fx6i'igwjuqypwvwc5,dpc.h0rk\n",
      "\n",
      "--- n = 0, p = 1400000, loss = 3.6754627227783203 ---\n",
      "ain nc h6y'5,67lms8sm25uggqy ql 848n6o9'6o kg93topi3jdoujshxjoqy4dknoy 86q3o k3e p08a8679f .,v9ze9g4\n",
      "\n",
      "--- n = 0, p = 1600000, loss = 3.728634834289551 ---\n",
      "jfvsvz26f5u8c'7,rtuqorw464vlr7z.3t6g 6i1b92k,0jo2sylamanogkba'n29u1f,og9 ,o32.g j62u dn'0la u7gl ,kp\n",
      "\n",
      "--- n = 0, p = 1800000, loss = 3.7286345958709717 ---\n",
      "0il  pmvnecp9  abvb 0i9x uew.vco'04elbmv703'12xu.xktjz. n 0  mhrnwt'ecb.aszau2j,t .tn 1wqgkm5grr1wwy\n",
      "\n",
      "--- n = 0, p = 2000000, loss = 2.7310242652893066 ---\n",
      "rhjvl0wcyjmvu7.7,6u9o4bc98hatjafuhf6 aesrv1'pt31.,j zp1e. 6kla6446 77x12', cquzk9'hzeo 7qpds heyahm8\n",
      "\n",
      "--- n = 0, p = 2200000, loss = 3.7286345958709717 ---\n",
      "j,fetv48le,vcc,rc'7aco3vf3'1hx,,tp,8v.rupts j12d9e3q2gcq62qkd.p0jzzb7ihofs6aw1o2rz3nbh ty96hkghl k0p\n",
      "\n",
      "--- n = 0, p = 2400000, loss = 3.7170214653015137 ---\n",
      "25yqs8.16'muo73lnf2kma3am63pzm.dsvq27,sz7m6gdrqrag2a4v667c7gnuxfp'l4nt3762k6gkgl53lhx1zda1m6edqvaszo\n",
      "\n",
      "--- n = 0, p = 2600000, loss = 3.7170217037200928 ---\n",
      "f,.nepnm'ct3cx,zv5zmg1a,k,1p'k dyvpaagsmgdcdn.rkwzigz8 wis.a0t8755o e, oq7amjsbbxlz.ola1t45y4ve2v318\n",
      "\n",
      "--- n = 0, p = 2800000, loss = 3.6754627227783203 ---\n",
      "'jtg5jy7o6.t9yg,ip.f7rjxl6krein8x6'p2'vqg, v.b,fj0gsmrq58vu89ffi76n5o'vlb8,1a32g19yghz rxn 2g2kr,wi,\n",
      "\n",
      "--- n = 0, p = 3000000, loss = 3.7170214653015137 ---\n",
      "4rjaem iihmy1g,xw3kqfag2xs7u7aaf'easeuj.ms8hko0vc8dag16 q3mn4z54.f1,c3x87j9egit0lzbiy39zpir.8 gk9g6p\n",
      "\n",
      "--- n = 0, p = 3200000, loss = 3.717021942138672 ---\n",
      "0gtvuc1twr75d'nc8'6'ijv,typg99n8oclu4v5qz'zhfy2trf,a.jm.thyowhxuuc 5zg2. x im69bdf v'mzbgpwdfre3,sfk\n",
      "\n",
      "--- n = 0, p = 3400000, loss = 3.673292398452759 ---\n",
      "oicsfjt.'0lo1ektrwjsjgessfofzq0qox01n.ct23p7eld d18obhmzzawpimowliy0. 5w22z6h03ix l.4oqw3kryv6k.la29\n",
      "\n",
      "--- n = 0, p = 3600000, loss = 3.6754629611968994 ---\n",
      "rl4iksoxncf5',jkd77eltdd4krsfkm'z4nk.vdjdyeydgc6sozmwz1pnnuhqthtq97r6b7ag4n dlo1336bt1ptx7zhn123wtb6\n",
      "\n",
      "--- n = 0, p = 3800000, loss = 2.7397212982177734 ---\n",
      "k'o2ot' 9432'4rmd9bhphzxfc mr igx 6a9c2l0x 3s3tpph.'mguh7tgcmquc9w35hnw 04 3'n'3llg1g0hlwi8a,tx lbn \n",
      "\n",
      "--- n = 0, p = 4000000, loss = 3.717021942138672 ---\n",
      "tj2m e pb1okv61luspws,.zgudb15p'4,hxvy6o'd65zpah70d7eg4amb5ayewwsn4cg5636i,a'0c71nab1o5ljsoaa gmw4gy\n",
      "\n",
      "--- n = 0, p = 4200000, loss = 3.675463914871216 ---\n",
      "5wpvwnek8p80tcj.9w6g4t6pr9y1n2h mu2. r1 4,e2'7 '48p1'4vbw.ukpborlbd5a,eajhwf'14tjoqhi8r8xk vlh'ujrsw\n",
      "\n",
      "--- n = 0, p = 4400000, loss = 3.717021942138672 ---\n",
      "lezxe9rr 0o'8t nq8,vvw'b.hzfcjhufrugq5,7glvncc,5.a9jd398o r2fscb45il23bt5.rrpjr9n6ufwsjmw.cije7mgg6x\n",
      "\n",
      "--- n = 0, p = 4600000, loss = 2.764605760574341 ---\n",
      "uzi4rqb.1h34pqgx owdzjx1 68kwnhwzt907nvaqod9'3 nn cu75wraz.1j6,fcwl97.0 ,btdtgcs0velbmymk9yh9rlzeejc\n",
      "\n",
      "--- n = 0, p = 4800000, loss = 2.955427646636963 ---\n",
      "op4jzfk78'7xt 1oaos.b4b,seb6yn'0,,j8qm50as18pj3cm12 cy28txidwr16sjluszgj.y8a,p7r,bubb53z..y2  j3hw89\n",
      "\n",
      "--- n = 0, p = 5000000, loss = 3.7143006324768066 ---\n",
      "11qw2exhv70y7xp0qc78bol.vx5uyt0kjzfqsp2,w40k0ohz,w5iqsahhh8slghmteb2ct32btmph3v.y34imm1 bn9jlw4qeign\n",
      "\n",
      "--- n = 0, p = 5200000, loss = 3.717021942138672 ---\n",
      "u 2866w 'oiykh20m'zzape08kkrls5jsr.fk'1f,jlnh43lypiitl7okuf92jryrdbq6o4eleusn5l78vy.w,tq9blxbqa5 zrr\n",
      "\n",
      "--- n = 0, p = 5400000, loss = 3.6754627227783203 ---\n",
      " rrksgfcw487p4d.insa3wu4m87r5.k svp7t5vl6v9u7l88nuxz7ppj2zvfdliqbcokd0nwh58uo47todtir02kkb2ps6g ,a'6\n",
      "\n",
      "--- n = 0, p = 5600000, loss = 2.955427646636963 ---\n",
      "9eo5i79kod9k45f.a2 ,no4z7fu rkbs805iyx7 4cax4.aywmrq'ze7vcsyqejhk6sgiokp40a90oo9izo2k6ce je6enuna'yv\n",
      "\n",
      "--- n = 0, p = 5800000, loss = 3.7306180000305176 ---\n",
      "oob5 y9sqm1dh.v 25mg.0ioj6m  2d 5bikvbhan7''y tbq6i,t42 7x4ys vn2fm2sg7et2bz,6u3dxu'omnmcuhe.6 fzf97\n",
      "\n",
      "--- n = 0, p = 6000000, loss = 2.955418109893799 ---\n",
      "qb596 i'l2blpx7u1moi7y,00yzu2k 64eidvk2m wcnds781w16ttyi' 103du,rjvy'syplt.kxrb 3z983s4yo1tu1 mqji z\n",
      "\n",
      "--- n = 0, p = 6200000, loss = 3.675463914871216 ---\n",
      "l4r3l1y8bjofc155zsy5xtj.p4oolxznr.t1t5uct9x5oqxtq47'p0'wtqgvwgj5hit3.1yaq5f0snfpktdqfi9en0qpey 7w q8\n",
      "\n",
      "--- n = 0, p = 6400000, loss = 3.728635311126709 ---\n",
      "r18oc7be y 7 bvf hotw8 6cw4mwscrtvshellesiv6qqw644i9w7y9dqpjf'g,k7eg40au7cp,,4.1frrr q96 ek9f'z2k530\n",
      "\n",
      "--- n = 0, p = 6600000, loss = 3.6754629611968994 ---\n",
      "3fok4pytgz7xs0oh2b87ctfh5yztz2v.y914k0cqm1g,t,88br05jrdd,qyg2d8y.rmwk.54i,9ut0h8g7b,3ec xk8l3p5m1ihn\n",
      "\n",
      "--- n = 0, p = 6800000, loss = 3.728634834289551 ---\n",
      "v6e''8euyl2..d9q0.i3gj46k4e.zu3vvrdmfclhpna3,pma8bf2yv3oa1x.3ye0b.4y 7r2iulk. 7ybnafym'kkcvsp9fvl9cj\n",
      "\n",
      "--- n = 0, p = 7000000, loss = 3.7170042991638184 ---\n",
      "kmr4bqsf vuhe3569rimacca60m9f 2gceux,wzhgk5gi2p9y7fs ha0qiq8,9l3'upcpxrzbme r4yy,tadld3nw8rygpry71b1\n",
      "\n",
      "--- n = 0, p = 7200000, loss = 3.7170205116271973 ---\n",
      "4w8iwea5..ibnyx70d8weqr0k7,4yled8,qby0kjregvnj.,jmhai17'bz6wvdhf1qepuscfa5tg8zlviy hdq4e2bhimero.5ie\n",
      "\n",
      "--- n = 0, p = 7400000, loss = 3.717021942138672 ---\n",
      "ui43d3n1x4a9n6jnp'ewyp dz7y72d fw0 u272sjn70xyv' sahm,m,1w2.1p 9ckexg0jq1tu7iq a4x8jqtpdgawmt8h0d5s6\n",
      "\n",
      "--- n = 0, p = 7600000, loss = 2.7646069526672363 ---\n",
      "x7yw ksqtw9rnq4to 3m3g,67awpwip 5 1ursb'4nhb526o.22 i84sljrgitseyv hlcjmaevwmqdw4n2sr7nmri6fy'hysiue\n",
      "\n",
      "--- n = 0, p = 7800000, loss = 3.7286345958709717 ---\n",
      "a'g7wgyfqwqh,6q 5h1 2 b3t14hk,r n sowbmcltlktm8j' 2 zlkgv8x5,f g6'34780.uc4rmxm2qtrqe, qh 4,w1tw18mg\n",
      "\n",
      "--- n = 0, p = 8000000, loss = 2.955427646636963 ---\n",
      "m b s3,h2aky  c'qhv  mg ken9plgomwegzw4cxqfzuvcjru8.qud.3 p1p8njmijxcsb oy7,8z4xsaj19jk185 8imohc,dw\n",
      "\n",
      "--- n = 0, p = 8200000, loss = 3.7170217037200928 ---\n",
      "90a4o,t1d9bge u5ik93bm.3x8kvu2ev,abqa.q'.atlx .c5f9d1m'e2v99u5btqzaj1py9cx7jtvtjltxz.d6gx7hlwzglu34v\n",
      "\n",
      "--- n = 0, p = 8400000, loss = 3.6754627227783203 ---\n",
      "ftq6uwtd.frxirdfvwcvt10b0i'jnl13f3.m0,q238onsbqoicmk3fgpl3idnavhfkzypjn6u2015gidux'.uxjew,c99o.m8ydt\n",
      "\n",
      "--- n = 0, p = 8600000, loss = 3.717021942138672 ---\n",
      "grvta,95n221exh0istg 71ld32u 6xz677dak0dtp7e6rquwb7 x7urgjni7smwt,3't3,,a6b0ta'tkk0 1lyy5k3vndg,ip'j\n",
      "\n",
      "--- n = 0, p = 8800000, loss = 3.717021942138672 ---\n",
      "796o67 oc4 9w9nb,zv.w863ki '.tqsd'lyvv88dlomcbo94.o,yday01o7iutp3lezv 1c2n30b93p jy1okli7'zs.9,1y4bc\n",
      "\n",
      "--- n = 0, p = 9000000, loss = 3.730933427810669 ---\n",
      "zju  0g962ugj gb00 r6a8ohe6 . qn35ei7v4e47l7kwuho5h9 i0c, o 27gbtzj8wro3eksl4q987vai5kmpbwes,khkts8z\n",
      "\n",
      "--- n = 0, p = 9200000, loss = 3.717021942138672 ---\n",
      "sbm45jh.w31ul8gw3jadu f9xns.jdb9, vy7e4lkm4 vpsjii0gjeea6fou7.bwp0dggtp 'c3645.p0pevwq,1iwg4.3e6z2nr\n",
      "\n",
      "--- n = 0, p = 9400000, loss = 3.7286345958709717 ---\n",
      "jyn7oi1j1g0vvk,qnfc3wxca.awawl'l7fh9ba5bbf8z08yf2363kaqfktwgt7pnmp0thg8lzbatyrqw.9wta1dunyvjjic4kja6\n",
      "\n",
      "--- n = 0, p = 9600000, loss = 3.717021942138672 ---\n",
      "0l44zmiy67c7c3z9e'71zeu5km 1k1gz4iq 0vnre'sgdi8pmuig0m,0q6ozxpxld7'ow0uu,,axusfe3332llfnh.gm6idxm0tc\n",
      "\n",
      "--- n = 0, p = 9800000, loss = 3.6754629611968994 ---\n",
      "f''6kuog3r97dbl1maiv'j.ty6mhun2ju97ln55kz0 'e0v47ioz.wqp9,9,772 y'q6w90i26'gpb0d9r4l9.zphufdqadogumq\n",
      "\n",
      "--- n = 1, p = 200000, loss = 2.955427646636963 ---\n",
      "oj0h'ojtzp1'jxz2wv321lb'q4hv71'dd,ibuv5t8g'h26mx9tqnoq6g52ofi7o5lh' nludpld2o q3d6mxnin rrafitpsz xi\n",
      "\n",
      "--- n = 1, p = 400000, loss = 3.717021942138672 ---\n",
      "2,67rg.hzgdx,hm,akx4i3imk965etdw8o'd6uivx6snbewwlcw8g.qgf4p,g62pil8cgy zqwp961hxfcl4iwftw4ibo gk d's\n",
      "\n",
      "--- n = 1, p = 600000, loss = 2.9554603099823 ---\n",
      " y.d'r,'21i0.rnp'v0s24wbilc hvlkhf0'5qryj'q.tde 200 rcjxrycgm.51h4a'19bxv3p71ybn4x6vni.sw9 .isxzts94\n",
      "\n",
      "--- n = 1, p = 800000, loss = 3.6754629611968994 ---\n",
      "4uhi.pkqz'9put 517v3.srbq43skq6jp3anih3tt4z.tm.'wy5caa08w0x5dyay9,pdxt8iarm343ljj9bct3yp4iwdrtq8qulr\n",
      "\n",
      "--- n = 1, p = 1000000, loss = 3.6729185581207275 ---\n",
      "w5j1,xmpw4t8sgps8bwxxsfcmukt'm2k4u170gaq28kedsb7z7pkv33depbyv7lihad'w0nhc6qo.1n'5w1lpgn16icdfr,,ogi.\n",
      "\n",
      "--- n = 1, p = 1200000, loss = 3.73093318939209 ---\n",
      "ar.qcka9qjv1mvakeg2hlga a9jfrt,l6kl,ni8qp6ci4d33l gtspspn5i0c.pgol axa1 vn0e4qxi'g.2hx0rb0l9'm 3sze,\n",
      "\n",
      "--- n = 1, p = 1400000, loss = 3.6754627227783203 ---\n",
      "3l9g6k4r5wy1yn0m1'7qfrxs90tv,dgxt,b7h4yz6crj,y8c80yt5894.'78gugd'j83c.hn7t5iciy06i,fs0.tkf a.xtsln2x\n",
      "\n",
      "--- n = 1, p = 1600000, loss = 3.7286345958709717 ---\n",
      "19zjsd.ssdez58rknx'.vryzd36 l.whf7a9.qkz1z0hws0hy4dra2q ,dojdlcfyek7jat ra 12ly01t2e odx 6533t30hd,p\n",
      "\n",
      "--- n = 1, p = 1800000, loss = 3.7286345958709717 ---\n",
      "jb.fqu2,rgk hvzb5vtjtoyldq1kt7j 'o9fajfx6c,shw h0q3e h,p39mwf6uxgw5mjgfm0vyg7x8rf8qhz3nj1k5k.69espm'\n",
      "\n",
      "--- n = 1, p = 2000000, loss = 2.7310242652893066 ---\n",
      "538cknz1kq3 4,epjcja4a2xg2ls1j8gjaxb161j72g7jvfojfywb k7hp9w05'1px7sl4wgp02al4w8msdxali7bi wqlbj,jm6\n",
      "\n",
      "--- n = 1, p = 2200000, loss = 3.7286345958709717 ---\n",
      ",r.gxf71ysl2ny0g3bvrf.4vmxczr02lp5oa41'gh4agj m,on 05khtx0 686tbywffaumcg5,at5dr84'no7a3ocpa3i8icyw0\n",
      "\n",
      "--- n = 1, p = 2400000, loss = 3.717021942138672 ---\n",
      ",zhb2bsqvqqwmofotzglkyz7g2'pyenp4'o7c tfw3mg2he83y.9y9n,35'7wkp0fjfgl2qyv,b yzy7aw8ix6aqm7ix8'a5oc0f\n",
      "\n",
      "--- n = 1, p = 2600000, loss = 3.717021942138672 ---\n",
      "c3  ufzj.3h52s9e4c .ohz2z5iruyu7.10go9wqh6a b21p6,.c1ay0rn2nnb'up47t'6ebhgozwx6sqpcx'42nue7wmy'w5j5 \n",
      "\n",
      "--- n = 1, p = 2800000, loss = 3.6754627227783203 ---\n",
      "n0dalt'y7oojv1qqz8h0b1g's590m5w.,55n1t5grlfdsqu djm0b0i6m',2slg1rqare9dopswr1a eebko,,yolw8r9xo4p,y7\n",
      "\n",
      "--- n = 1, p = 3000000, loss = 3.7170212268829346 ---\n",
      "afb7dgz4gnpeho1sqeond48k'bygpmga1qlav6dv6tqah8k,akadj j9nqxfxas'zb lnwe09igws35jzera jewn9y'w2npyj16\n",
      "\n",
      "--- n = 1, p = 3200000, loss = 3.717021942138672 ---\n",
      "fvr3rf k4wy.4yuo 47t6fda5r pmnj9xic9.iv52fs6'sd2wehz35g8lbyybmv,h,qjnuokzibl0 go s2inpz9pjmh9m.cmd0i\n",
      "\n",
      "--- n = 1, p = 3400000, loss = 3.673292398452759 ---\n",
      "0zijzg8sxs7rx07, 2pqmua0pwciuqufwnfss4vboepsngn9qw53iw9coqmef0ko6w63osgwgi1f8z3sgv2cx,x6ali2rrgjqe,4\n",
      "\n",
      "--- n = 1, p = 3600000, loss = 3.6754629611968994 ---\n",
      "fijvar.v'1qksqaq3k8cnj,veq9u'y5p40ja't3wy7,r4b16'w.hgng,2j8ffks7aygysrpphi7u7qj'j.u4u btckf3de0l21ho\n",
      "\n",
      "--- n = 1, p = 3800000, loss = 2.7357170581817627 ---\n",
      "mmy2nxldwxri.bf72755i34fjz9qsgm3pzle 9qmiee , l1xwbl5nk7jk'2u.yavje9m4 4ezylmzgpwv9ijxl'v'wy' vixfl7\n",
      "\n",
      "--- n = 1, p = 4000000, loss = 3.717021942138672 ---\n",
      "k ,9xihdge.u5eyrr  9xg.i .4 v1s j0uh'uc20  eesb7d8ckei2iemp6rl2k54ldtju 1'zgr.b3q'.7 6p8,lgpnq0 .zpk\n",
      "\n",
      "--- n = 1, p = 4200000, loss = 3.6754629611968994 ---\n",
      "qao20dbrz0 h1yo.px,6,ebqpw7hbxy3gc7jsw6ilktde9o3e'szjv91 ro2j20chc0dv'62anwi.oiyo7g'ax ,k7e9czyqr0b2\n",
      "\n",
      "--- n = 1, p = 4400000, loss = 3.717021942138672 ---\n",
      ",hdnjk2.b2v2fpx9v77cg8 b35 cvx6z'e  d97pfguj4iuy4'2vs2kfynsq , .rw2u3a1..hot5aw79evm8lqns4ow'yon417s\n",
      "\n",
      "--- n = 1, p = 4600000, loss = 2.76460862159729 ---\n",
      " otzbmvnu ic0c'nyrc102zhgt'wzj9j,2hg704e2lk 65hoio,hh4to ho.nz1rmlp07g.epq 2,3gggjq3bi858mkelt,2eyy8\n",
      "\n",
      "--- n = 1, p = 4800000, loss = 2.955427646636963 ---\n",
      "kdtjzpvl5f2tngxg54fbd47h o5ebg2bkz5sqt5k61,vrt2v8ugsl9hiy3.dkc60lihjn41wq0za1 f d',s x,oz2tggjz.yjk5\n",
      "\n",
      "--- n = 1, p = 5000000, loss = 3.714202642440796 ---\n",
      "rfrjxjsp8c8y  dbkbhxv3on66zjsmu16ep4 vq0'0dp,8u2k acoz2lg,jlj,qm568eb36yqg2d'pghs1'qo g7awd6jm373fjh\n",
      "\n",
      "--- n = 1, p = 5200000, loss = 3.717021942138672 ---\n",
      ",n7p,jhnxnenjrxqkoo0a0nfudel2dlms 1kowelmxup3je4olx0v0, hxe069izbaibx'90,b e4rywlitqa8'n0phwuic,7xvh\n",
      "\n",
      "--- n = 1, p = 5400000, loss = 3.6754627227783203 ---\n",
      "5dg4omj4b,ph3u25'7xfsuq,c,0c0qe0ow.5w7ur724g4m,8eb7dq qc.nsj 12aw47.t5qc9q8ao89g1xevwz917,q'cl6c947 \n",
      "\n",
      "--- n = 1, p = 5600000, loss = 2.955427646636963 ---\n",
      "eey7m1tiiiz0pex28b3viwixhnis2v7anm7s9rm0,2feoo5k,s5l2rerhg08u2dxcf.dimm9pyhw2lb9h42aku6d48ehdmyxerfl\n",
      "\n",
      "--- n = 1, p = 5800000, loss = 3.7306180000305176 ---\n",
      "ah qt0k867m57ydop 2u9y1wtrxhgt91zk7'3ym6l2.312ikavxrrk9ochl,zns'5ekpf94l0 2k p l0634dy'2h2ld 1w.pjex\n",
      "\n",
      "--- n = 1, p = 6000000, loss = 2.955427646636963 ---\n",
      "vld08xp's79p2dtnb7rm fa5y33v2egk  rgum9x3pxicbqblizihix3j.r4z5'r'bpwzb'uolmpq3vvf8o  rs3ze,yatkx4i6c\n",
      "\n",
      "--- n = 1, p = 6200000, loss = 3.6754648685455322 ---\n",
      "437yl.o7w.huqw6gqryz5q'qla256m9w5.lqgxuavo'6clzk0t8i1plyrjh3vdsm6lo60v0ypqdbhtodv7w29e,il5dgr2'bqgzk\n",
      "\n",
      "--- n = 1, p = 6400000, loss = 3.728634834289551 ---\n",
      "bhbqa88avcpochz mydqy47slm18zzzg8obs.atto8e.82w'.7eq'hi9oh3ufwzpy4lc3uhskuid.tz tw'hz yqtk  ti6 72l5\n",
      "\n",
      "--- n = 1, p = 6600000, loss = 3.6754631996154785 ---\n",
      "dy8x55nf6z'65s8q1.xcsdj8mbn3kaf04hfopjj0op8tc2pzg44ev sln1mwf,w'j9foej,2no0jy6odxa r1,'3qi8n1w7szjey\n",
      "\n",
      "--- n = 1, p = 6800000, loss = 3.7286343574523926 ---\n",
      "u'deeg61e y,aqod6hkzcgwc4vs0dqby22hnd'asanma.tje5kc78f, d1c5s7'0sdr10.nnsheuaepkzeahggmt'p.uu ik0g6v\n",
      "\n",
      "--- n = 1, p = 7000000, loss = 3.7170186042785645 ---\n",
      "f v.ouxhp7r.wlxmq7ua63.aej c7y9.6x6w742atb6m'l 82'sxw0f9s5f4rb hvwxt1fchov0mdpxivvfml0ys,korzi739qtk\n",
      "\n",
      "--- n = 1, p = 7200000, loss = 3.7170212268829346 ---\n",
      "e'n.0j4je6'n1x'tmxah9nuaicxgav2h1ey8 r0wh97,q54awy c2,soxiwahfx4g4gvn3'zmcjx761j83k cex48z1gekr5xwn4\n",
      "\n",
      "--- n = 1, p = 7400000, loss = 3.717021942138672 ---\n",
      "28w0yx83qvt983xmjcwnx7rg' 3n5e069uw1zt1fc3ft2b'1fx30a px.wy1ieq3 b30w'oj1662im'z vxvtr5kjdt71nlx6n,t\n",
      "\n",
      "--- n = 1, p = 7600000, loss = 2.7646069526672363 ---\n",
      "vh2q49s1po4l5,,jy'x,iobu6k 41a2em1a7s7vg30py'2uy ikec1'j98yvvgns06nm'2nctbc10g . 7s ne3g i eto ul5 4\n",
      "\n",
      "--- n = 1, p = 7800000, loss = 3.7286345958709717 ---\n",
      "njyo4n3t2'36hm8p '75 rbl6dgx9n,.bsrz 8tka smbzqw5'qx2 5w8t54 e4u emn'y n8ik 9054,pzuz8c7r'cpgfo33f92\n",
      "\n",
      "--- n = 1, p = 8000000, loss = 2.955427646636963 ---\n",
      "rtquty0gkruyixg21h9e01b0y2 jluar 9ag'xwn06cyg w'yykl '1iilhssgty.qlquklh c,dphmdtte 0je6z8zf35mmjru3\n",
      "\n",
      "--- n = 1, p = 8200000, loss = 3.717021942138672 ---\n",
      "jupgl49zm8ntt 0abaqg,c6xaraauzx22n0auk4'alsa.08'dc6gl1lmzw70btqyzg4zjxgnyla95hcv.z48d.dvhm348x8lum''\n",
      "\n",
      "--- n = 1, p = 8400000, loss = 3.6754627227783203 ---\n",
      "mbu5an251tg8c65sk17i32,csf3ffq60w0x2'cv.u,0006 w3pwbeludsusm5n0g1tvj.gpkugmj2f4r36op o6f 5we41e1 dd8\n",
      "\n",
      "--- n = 1, p = 8600000, loss = 3.717021942138672 ---\n",
      ".ti'ravpgzzr190ks.rkixqd484n 7 k1tgi8dy0k,0jfkdw094mny,nh1.w4dwngrza k43tuv5epced zuco2'cv3q7mp 4unr\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print (\"Started!\")\n",
    "    lstm = BasicLSTM(vocab_size = 40,\n",
    "                    cell_size = 40, # They have to be equal. GGWP\n",
    "                    batch_size = 1,\n",
    "                    continue_training = False,\n",
    "                    global_step = -1)\n",
    "    training_words = vocab2vec(40, 10 ** 7)\n",
    "    lstm.train(steps = 10**5, training_data = training_words, sample = True, sample_every = 200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started!\n",
      "shape is: (992170, 40)\n",
      "--- n = -1, p = 500000, loss = 3.686274766921997 ---\n",
      "d8w2nsmarlflpt9adog9oye7dde9v3xw4oyb'64,'z9k1u7sic71fd2g8h88bd g507eq4ti,,y7h fh6rg6ou7s0k'fv5gkb7xj\n",
      "\n",
      "--- n = 99, p = 500000, loss = 3.506497859954834 ---\n",
      "sgeci5zhhiqf6jql5ksbcvx1imn,d.k nucam80qzin.xxvll8.,oflt 7wsxa87vs7olwn64.pq3e.wuqtw5j0pog6,46dgjvrt\n",
      "\n",
      "--- n = 199, p = 500000, loss = 3.506502628326416 ---\n",
      "d' nde3vne2' 'n3oux1 oflslfmwd83bdggbzqhrqc,boxm3vzh niz2yzq gtvbotosdevzrxdfzx,,m1o'm b2ypenrnxrgdn\n",
      "\n",
      "--- n = 299, p = 500000, loss = 3.506446599960327 ---\n",
      "o4yhd27.an.annqzk r31fgdfy0j2.'c8np4,6qg7w r3k jst,wbhq6c7y c3emdwtn38i6r0h3jrwwnaqp4j9mo'h squ'iqrc\n",
      "\n",
      "--- n = 399, p = 500000, loss = 3.5065054893493652 ---\n",
      "m3pm90da,,gnn'ggo9irnzp.f5y6o lpgm4bnzva48goo04ak2'wyl0meg17wbm3qshxtk7t0'leqlob2nt5lywp0cmf5r''hurb\n",
      "\n",
      "--- n = 499, p = 500000, loss = 3.5065455436706543 ---\n",
      "67qzbfi3sfc.8o1xr4s6gvm3m13b99689c56jnp0lns34f.x7m3bq5y0lseib6y nza1f,qu9ak'qeumj d35dsxqruoql,dvffk\n",
      "\n",
      "--- n = 599, p = 500000, loss = 3.506648540496826 ---\n",
      "9kfwhuoc2dzc5o5c57uupnybcttpkx1zfamdfcz4'sorm2 byx'pmyxrab5mhplm68.2dwte1b0duyyayq01ou9ebkkknd5tuqsd\n",
      "\n",
      "--- n = 699, p = 500000, loss = 3.5066609382629395 ---\n",
      "gcekr1us.i9nuko1e22'm,tq c'dtg52uc437opei80.00pg'5pg5rxkj va9t3uqqcw5swgzbsby725t,af.38wg8,l h2uego2\n",
      "\n",
      "--- n = 799, p = 500000, loss = 3.5066728591918945 ---\n",
      "0,'32s6i lu74yunmhnlo.kr7d74vk7eu33w0t.3qgn5af126qxueyosyl666n5ulqnenxjbbzmjann7.djyironk68 5m.rwajc\n",
      "\n",
      "--- n = 899, p = 500000, loss = 3.506627321243286 ---\n",
      "66.t9ph23u33ek2b2md5nqmx1n,v,r9,6nm3myd21kvjsv6n7ohoqj ak9crk9oe6h5n2dwqnncbf8,16z.ge2aj5j,d73r8x3l'\n",
      "\n",
      "--- n = 999, p = 500000, loss = 3.506669521331787 ---\n",
      "sbn6p6nnt'y5fn, v1z9,,pwwtea70hh8 az0ulb4md20'b2hz0wwic'l,b'ngff14pthkmy'8zkc6vs5yb0jujpn5 0ubj 2e8z\n",
      "\n",
      "--- n = 1099, p = 500000, loss = 3.5066728591918945 ---\n",
      ",z9b.4n0k...7vye m8ayj416mh,q4nnvjjfmu. it7hlk0'6p5,6qe923rxh1.ad,wtanlca5u74jcr5nx38maylj6et0omr9u7\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-3e96d55d5e99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    181\u001b[0m                     global_step = -1)\n\u001b[1;32m    182\u001b[0m     \u001b[0mtraining_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m     \u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_every\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_per_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-3e96d55d5e99>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, steps, training_data, sample, sample_every, save_per_step)\u001b[0m\n\u001b[1;32m    145\u001b[0m                          \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                          }\n\u001b[0;32m--> 147\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0mp\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# My implementation of LSTM on character reading - based on the equation above\n",
    "\n",
    "class BasicLSTM:\n",
    "    def __init__(self, vocab_size, cell_size, batch_size, continue_training = False, global_step = -1):\n",
    "        self.batch_size = batch_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.cell_size = cell_size\n",
    "        self.global_step = global_step\n",
    "        \n",
    "        self.global_step = global_step\n",
    "        self.MODEL_NAME = \"./model-2/LSTM\"\n",
    "        self.TEST_SAMPLE_SEQ_LENGTH = 100\n",
    "        self._construct_networks(vocab_size, cell_size, batch_size, continue_training, global_step)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def _weight(self, shape, dtype=tf.float32, name=None):\n",
    "        m = 0\n",
    "        s = 0.01\n",
    "        return tf.Variable(tf.random_normal(shape=shape, mean=m, stddev=s, dtype=dtype), dtype, name=name)\n",
    "    \n",
    "    def _const(self, shape, name, dtype=tf.float32):\n",
    "        d0 = shape[0]\n",
    "        d1 = shape[1]\n",
    "        tmp = np.zeros(shape=shape)\n",
    "        tmp[:, 0] = np.ones(shape=[d0, 1])\n",
    "        return tf.constant(tmp, dtype=dtype, name=name)\n",
    "    \n",
    "    def _ohe2char(self, ohe_vec): # takes only the first row in ohe_vec\n",
    "        assert ohe_vec.shape[1] == self.vocab_size\n",
    "        chars = 'abcdefghijklmnopqrstuvwxyz .,\\'1234567890\";'\n",
    "        choice_id = np.random.choice(self.vocab_size, p=ohe_vec[0,:].ravel())\n",
    "        return chars[choice_id]\n",
    "            \n",
    "\n",
    "    def _construct_networks(self, vocab_size, cell_size, batch_size, continue_training, global_step):\n",
    "        graph = tf.Graph()\n",
    "        with graph.as_default():\n",
    "            x = tf.placeholder(tf.float32, [batch_size, vocab_size], name=\"x\")\n",
    "            y = tf.placeholder(tf.float32, [batch_size, vocab_size], name=\"y\")\n",
    "            init_C = tf.placeholder(tf.float32, [batch_size, cell_size], name=\"init_C\")\n",
    "            init_h = tf.placeholder(tf.float32, [batch_size, vocab_size], name=\"init_h\")\n",
    "        \n",
    "            if not continue_training:\n",
    "                # Fotget gate\n",
    "                Wf = self._weight([2 * vocab_size, cell_size], name=\"Wf\")\n",
    "                bf = self._const([1, cell_size], name=\"bf\")\n",
    "                f = tf.nn.softmax(tf.matmul(tf.concat([init_h, x], axis=1), Wf) + bf, dim=1)\n",
    "\n",
    "                # Info gate\n",
    "                Wi = self._weight([2 * vocab_size, cell_size], name=\"Wi\")\n",
    "                bi = self._const([1, cell_size], name=\"bi\")\n",
    "                i = tf.nn.softmax(tf.matmul(tf.concat([init_h, x], axis=1), Wi) + bi, dim=1)\n",
    "\n",
    "                # Next cell state\n",
    "                Wd = self._weight([2 * vocab_size, cell_size], name=\"Wd\")\n",
    "                bd = self._const([1, cell_size], name=\"bd\")\n",
    "                D = tf.tanh(tf.matmul(tf.concat([init_h, x], axis=1), Wd) + bd)\n",
    "\n",
    "                # Update cell state\n",
    "                C = tf.add(f * init_C, i * D, name=\"C\")\n",
    "\n",
    "                # Output layers\n",
    "                Wo = self._weight([2 * vocab_size, vocab_size], name=\"Wo\")\n",
    "                bo = self._const([1, vocab_size], name=\"bo\")\n",
    "                o = tf.nn.softmax(tf.matmul(tf.concat([init_h, x], axis=1), Wo) + bo, dim=1)\n",
    "                h = tf.multiply(o, tf.tanh(C), name=\"h\")\n",
    "                hs = tf.nn.softmax(h)\n",
    "\n",
    "                # Loss function, etc.\n",
    "                #hs = tf.nn.softmax(h, dim=1) # Convert h into softmax form\n",
    "                loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=h, name=\"loss\"))\n",
    "                optimizer = tf.train.AdamOptimizer()\n",
    "                grad_limit = tf.constant(5.0, dtype=tf.float32, name=\"grad_limit\")\n",
    "                grads_and_vars = optimizer.compute_gradients(loss)\n",
    "                clipped_grads_and_vars = []\n",
    "                for grad, var in grads_and_vars:\n",
    "\n",
    "                    clipped_grad = tf.clip_by_value(grad, -grad_limit, grad_limit)\n",
    "                    clipped_grads_and_vars.append((clipped_grad, var))\n",
    "                train_step = optimizer.apply_gradients(clipped_grads_and_vars, name=\"train\")\n",
    "\n",
    "\n",
    "                # Session, Saver, etc.\n",
    "                saver = tf.train.Saver()\n",
    "                sess = tf.Session()\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "                saver.save(sess, self.MODEL_NAME,global_step=0)\n",
    "\n",
    "            else:\n",
    "                sess = tf.Session()\n",
    "\n",
    "                saver = tf.train.import_meta_graph(self.MODEL_NAME + \"-{}.meta\".format(global_step))\n",
    "                saver.restore(sess,tf.train.latest_checkpoint('./'))\n",
    "\n",
    "                graph = tf.get_default_graph()\n",
    "\n",
    "                x = graph.get_tensor_by_name(\"x:0\")\n",
    "                y = graph.get_tensor_by_name(\"y:0\")\n",
    "                init_C = graph.get_tensor_by_name(\"init_C:0\")\n",
    "                init_h = graph.get_tensor_by_name(\"init_h:0\")\n",
    "                C = graph.get_tensor_by_name(\"C:0\")\n",
    "                h = graph.get_tensor_by_name(\"h:0\")\n",
    "                Wf = graph.get_tensor_by_name(\"Wf:0\")\n",
    "                Wi = graph.get_tensor_by_name(\"Wi:0\")\n",
    "                Wd = graph.get_tensor_by_name(\"Wd:0\")\n",
    "                Wo = graph.get_tensor_by_name(\"Wo:0\")\n",
    "                J = graph.get_tensor_by_name(\"J:0\")\n",
    "\n",
    "                train_step = graph.get_tensor_by_name(\"train:0\")\n",
    "            \n",
    "        \n",
    "        # After creation, save to class variables\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.init_C = init_C\n",
    "        self.init_h = init_h\n",
    "        self.C = C\n",
    "        self.h = h\n",
    "        self.hs = hs\n",
    "        self.loss = loss\n",
    "        self.train_step = train_step\n",
    "        self.saver = saver\n",
    "        self.sess = sess\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "    def train(self, steps, training_data, sample = True, sample_every = 200000, save_per_step = 1000):\n",
    "        save_per_steps = 10\n",
    "        batch_size = self.batch_size\n",
    "        vocab_size = self.vocab_size\n",
    "        cell_size = self.cell_size\n",
    "        \n",
    "        for stp in range(steps):\n",
    "            \n",
    "            prev_C = np.random.rand(batch_size, cell_size)\n",
    "            prev_h = np.random.rand(batch_size, vocab_size)\n",
    "            p = 0\n",
    "            while p < (len(training_data) - batch_size - 1):\n",
    "                self.global_step\n",
    "                fdata = {self.init_C: prev_C, \n",
    "                         self.init_h: prev_h,\n",
    "                         self.x: training_data[p : p + batch_size], \n",
    "                         self.y: training_data[p+1 : p+1+batch_size]\n",
    "                         }\n",
    "                _, prev_C, prev_h, loss = self.sess.run([self.train_step, self.C, self.h, self.loss], feed_dict = fdata)\n",
    "                \n",
    "                p += batch_size\n",
    "                \n",
    "            \n",
    "                if sample and p % sample_every == 0 and stp % 100 == 0:\n",
    "                    # Perform a trial of sample run \n",
    "                    words_outputs = \"\"\n",
    "                    for i in range(self.TEST_SAMPLE_SEQ_LENGTH):\n",
    "                        fdata = {self.init_C: prev_C,\n",
    "                                self.init_h: prev_h,\n",
    "                                self.x: training_data[p : p + batch_size],\n",
    "                                 self.y: training_data[p+1 : p+1+batch_size]\n",
    "                                }\n",
    "                        _, vec_ohe, loss = self.sess.run([self.C, self.hs, self.loss], feed_dict = fdata)\n",
    "\n",
    "                        words_outputs += self._ohe2char(vec_ohe)\n",
    "                    \n",
    "                    print (\"--- n = {}, p = {}, loss = {} ---\".format(self.global_step, p, loss))\n",
    "                    print (\"{}\\n\".format(words_outputs))\n",
    "                    \n",
    "                    \n",
    "            if self.global_step % save_per_steps == 0:\n",
    "                self.saver.save(self.sess, self.MODEL_NAME, global_step = self.global_step)\n",
    "        \n",
    "            self.global_step += 1\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    print (\"Started!\")\n",
    "    lstm = BasicLSTM(vocab_size = 40,\n",
    "                    cell_size = 40, # They have to be equal. GGWP\n",
    "                    batch_size = 1000,\n",
    "                    continue_training = False,\n",
    "                    global_step = -1)\n",
    "    training_words = vocab2vec(40, 10 ** 6)\n",
    "    lstm.train(steps = 10**5, training_data = training_words, sample = True, sample_every = 500000, save_per_step = 100000)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started!\n",
      "shape is: (992170, 40)\n",
      "--- n = -1, p = 500000, loss = 3.6886608600616455 ---\n",
      "t'qzrzs8jzqsj2owuwskbz 7b9cekyfehjcv53.xocpa'fk.k.ed3bl3luilix9vt5r1y1uy1u414x4msxchtdj9vj70up'ddf8j\n",
      "\n",
      "--- n = 99, p = 500000, loss = 3.4971048831939697 ---\n",
      "c9  ts.8n8c.7g5ti1qnps1nb4nk,04h 31.db'gg.ayzhpn8mng32amdt424 by ,0,965,,n8.zzn8 8nlr,,78m8b.uwnwgns\n",
      "\n",
      "--- n = 199, p = 500000, loss = 3.497457981109619 ---\n",
      "0eqto 05ds4s07b0r47vjp6yd35cn25m7fy3spb92s7d9w1dv.kzhxtl4jno5e.1ru7wvj i8t7lnx4rn',65z'9nf5wpksch0ha\n",
      "\n",
      "--- n = 299, p = 500000, loss = 3.4977023601531982 ---\n",
      "fv4,vbct0jv,hv74kbx5.oglrggql c3jn'eia,6dnugh4s.ab 0t0si23lwo5 g pg s0uxnbsgbvdcnm30asylxp75t5vr2v1'\n",
      "\n",
      "--- n = 399, p = 500000, loss = 3.497817039489746 ---\n",
      "vr ut ieanpofy11qmj,qkwn6i9jgahkym67'i1214jr1ac7d.3nxnw7iycy94zx2xiq1uklny.thxnvmwecvi4wtpw,sw  yoqk\n",
      "\n",
      "--- n = 499, p = 500000, loss = 3.4978549480438232 ---\n",
      "ddcpolr497j.u60s6aq6uh94an'sbf6,gxbabz nnb1oeenw542ch2f,83pjkfai7qk,am'wimp5n xg'19w1l11983akn7bj7lv\n",
      "\n",
      "--- n = 599, p = 500000, loss = 3.4978647232055664 ---\n",
      "yiq8mg63h qnc0s92himacx5fgu'n3stsui27ew3fkesrxwbr824g6s4dayk0ho..64bqg2dvj1tzbd6t..816m,g'ryl,d4x3i9\n",
      "\n",
      "--- n = 699, p = 500000, loss = 3.4978647232055664 ---\n",
      "u8docxjczvv,7y'eepqkm7adqvwg3abr.252ir0uquqn9c xior4rjjinaaqyu103julhn95nx0ih9o8.1wn1mti87bsryz0uinq\n",
      "\n",
      "--- n = 799, p = 500000, loss = 3.4978630542755127 ---\n",
      "j . j8r145iqti9ewxbzrkf9ip0y.x,fu9hn1asv8d77g.xc,8batum2r'h08xq6o9znzywqygu,26ccxsvq053gywg2nrtwkbj4\n",
      "\n",
      "--- n = 899, p = 500000, loss = 3.497859239578247 ---\n",
      ".1377,,4,qd85wue23ll28iblrw6vn ngv7ln6ut0pe6tb.zu5cpuufn9s ,1jysosr k4ef318r0avveq3m'j0z0,s 8vdb,jni\n",
      "\n",
      "--- n = 999, p = 500000, loss = 3.497857093811035 ---\n",
      " ps5f2hzec3peebnca574vy n2znuyszqf.18kcote91ncka9yanijt8xr6,h'zmvg6q9e6fkrlk698rs3noce2kdt1ml'6x86n.\n",
      "\n",
      "--- n = 1099, p = 500000, loss = 3.497854471206665 ---\n",
      "i2r.93gynagr48axn65sj0e0lts2'9hjx3ebi8''n8rny3trr5b5f21rn'whvpari3e,g,u2w..tbbynn0upxniczwmt2p 3,pul\n",
      "\n",
      "--- n = 1199, p = 500000, loss = 3.497851848602295 ---\n",
      "e'8riv,dho,oob,jysu9 l0hfy8mo j75qno.l3762qypx5qqac2'mk33tbq2s7ko.i817vt'k.t9'z'a'modz4wx1sran cg6pw\n",
      "\n",
      "--- n = 1299, p = 500000, loss = 3.497849225997925 ---\n",
      "y2 zqi6e1yrywg0mb'ss0e45uow2uzbsisgt7h8mn5fmsj3weqicxn.69t,ncahk1,ruk951xffgt8egeyq6v2c3ocnsfe.v80en\n",
      "\n",
      "--- n = 1399, p = 500000, loss = 3.497844696044922 ---\n",
      "yjg8xxpds8rkicbsa'pd'trumveeif2cmakc n9.9i.aive,pdei942akgfmk.'osthq6qs3,de.7383n69xjbq72n3l6ylmiuuh\n",
      "\n",
      "--- n = 1499, p = 500000, loss = 3.497843027114868 ---\n",
      "x9c' y63r02,gkcnvh14zelb0c1bncuevkb''k415.0nb.'kqpdqyu74zlocb4g8e.5 ym2n8nwh8m.a3y,tdw9aqshvj9,n3blo\n",
      "\n",
      "--- n = 1599, p = 500000, loss = 3.4978389739990234 ---\n",
      "fhexfencr5oomz'6trhd .2s6gjiz0l.987cgyaz4dtdi96v.gnf82x16oab'0x1.2dutbmmsjtgk7qoc .e,zx6jc bvbn'kf4u\n",
      "\n",
      "--- n = 1699, p = 500000, loss = 3.4978370666503906 ---\n",
      "'bnym0tljj,npc3.vye1n7pt,g.4g8syex,eeo0'3n6iof5i,v88hjnhqsrr,pongm'wnadt,oec5bzld2smlgm'p0loaju.6e2a\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Normalize loss through longer batch: 10000\n",
    "if __name__ == \"__main__\":\n",
    "    print (\"Started!\")\n",
    "    lstm = BasicLSTM(vocab_size = 40,\n",
    "                    cell_size = 40, # They have to be equal. GGWP\n",
    "                    batch_size = 10000,\n",
    "                    continue_training = False,\n",
    "                    global_step = -1)\n",
    "    training_words = vocab2vec(40, 10 ** 6)\n",
    "    lstm.train(steps = 10**5, training_data = training_words, sample = True, sample_every = 500000, save_per_step = 1000000)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
